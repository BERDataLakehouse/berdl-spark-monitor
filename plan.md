# Plan: `berdl-spark-monitor` JupyterLab Extension

## Context

Users have zero visibility into their Spark cluster from within JupyterLab. They can't tell if their cluster is healthy, if queries are running, if executors are spilling to disk, or how much memory is being used. The Spark Master Web UI exists but is an internal K8s service — unreachable from the browser.

Existing open-source Spark monitor extensions (CERN sparkmonitor, jupyterlab-sparkmonitor) are **incompatible** because they require a Scala SparkListener inside the driver JVM. BERDL uses Spark Connect, where the driver runs on a separate pod.

This extension creates two surfaces: a **status bar item** (ambient cluster health) and a **sidebar panel** (full resource monitoring), powered by server-side proxies to the Spark Cluster Manager API and Spark Master REST API.

## Scope (v1)

- Status bar widget + sidebar panel
- No inline cell widgets (future v2)
- New standalone extension (not added to CoreUI)

---

## Architecture

### Data Sources

| Source                | URL Pattern                                  | Auth            | Provides                                       |
| --------------------- | -------------------------------------------- | --------------- | ---------------------------------------------- |
| Spark Cluster Manager | `{SPARK_CLUSTER_MANAGER_API_URL}/clusters`   | Bearer token    | K8s deployment status (is_ready, replicas)     |
| Spark Master REST     | `http://spark-master-{user}.{ns}:8090/json/` | None (internal) | Cluster summary (cores, memory, workers, apps) |
| Spark App REST        | `.../api/v1/applications/{appId}/executors`  | None (internal) | Per-executor stats                             |
| Spark App REST        | `.../api/v1/applications/{appId}/stages`     | None (internal) | Stage progress, I/O, spill                     |

Neither is browser-reachable. All go through **Tornado proxy handlers** in the Jupyter server extension.

### Proxy Routes

| Frontend Path                                | Upstream                             | Purpose                  |
| -------------------------------------------- | ------------------------------------ | ------------------------ |
| `GET /berdl/api/spark-monitor/status`        | Cluster Manager `/clusters`          | Status bar data          |
| `GET /berdl/api/spark-monitor/cluster`       | Spark Master `/json/`                | Sidebar cluster overview |
| `GET /berdl/api/spark-monitor/app/executors` | Spark Master `/api/v1/.../executors` | Executor table           |
| `GET /berdl/api/spark-monitor/app/stages`    | Spark Master `/api/v1/.../stages`    | Active stages + history  |

Auth: `JupyterHandler` ensures authenticated session. Cookie forwarded as Bearer token to cluster manager. Username resolved from cookie via cached Auth2 call for Spark Master URL construction.

---

## File Structure

Base skeleton generated by copier (JupyterLab extension-template + kbase-jlab-overlay). Below shows **what we write/modify** — files not listed come from the template and need no changes.

```
berdl-spark-monitor/                    # Generated by copier
  pyproject.toml                        # MODIFY: add httpx dep
  package.json                          # MODIFY: add statusbar, react-query, statedb deps
  berdl_spark_monitor/
    __init__.py                         # MODIFY: env vars → page_config, register handlers
    handlers.py                         # NEW: 4 Tornado proxy handlers + setup_handlers()
  src/
    index.ts                            # REWRITE: plugin activation w/ status bar + sidebar + command
    types.ts                            # NEW: all TS interfaces
    constants.ts                        # NEW: IDs, polling intervals, sidebar icon
    api/
      sparkApi.ts                       # NEW: fetch wrappers for proxy endpoints
    hooks/
      useClusterStatus.ts               # NEW: status bar polling (always active)
      useClusterSummary.ts              # NEW: sidebar cluster overview (visibility-aware)
      useExecutors.ts                   # NEW: executor stats (adaptive + visibility-aware)
      useStages.ts                      # NEW: stages (adaptive + visibility-aware)
    components/
      StatusBarWidget.tsx               # NEW: compact status bar component
      SparkMonitorPanel.tsx             # NEW: root sidebar component
      ClusterOverview.tsx               # NEW: resource bars + worker count
      ExecutorTable.tsx                 # NEW: executor stats table
      SpillWarning.tsx                  # NEW: conditional spill alert banner
      ActiveStages.tsx                  # NEW: stage progress bars
      QueryHistory.tsx                  # NEW: recent completed/failed stages
      ResourceBar.tsx                   # NEW: reusable progress bar
      CollapsibleSection.tsx            # NEW: reusable collapsible wrapper
    utils/
      format.ts                         # NEW: formatBytes(), formatDuration()
  style/
    base.css                            # MODIFY: add all component styles
```

---

## Implementation Phases

### Phase 1: Scaffolding + Server Proxy

**Scaffolding** — Use the two-step copier approach:

1. Generate base skeleton: `copier copy https://github.com/jupyterlab/extension-template berdl-spark-monitor` (select `frontend-and-server` kind, python name `berdl_spark_monitor`, extension name `berdl-spark-monitor`)
2. Apply KBase overlay: `cd berdl-spark-monitor && copier copy /home/dauglyon/work/brainstorm/kbase-jlab-overlay .` (or from GitHub `https://github.com/kbase/kbase-jlab-overlay`)
   - This adds hatch-vcs versioning, KBase CI workflows (build, pr-prerelease, build-wheel), and Python >=3.10
3. Modify the generated files:
   - `pyproject.toml`: add `httpx>=0.28.0` to dependencies
   - `package.json`: add `@jupyterlab/statusbar: ^4.0.0`, `@jupyterlab/statedb: ^4.0.0`, `@tanstack/react-query: ^5.68.0`, `react-dom: ^18.0.0`. The base template already includes `@jupyterlab/application`, `@jupyterlab/apputils`, `@jupyterlab/coreutils`, `@lumino/widgets`, `react`.

**`berdl_spark_monitor/__init__.py`**

- Read `SPARK_CLUSTER_MANAGER_API_URL` and `BERDL_JUPYTERHUB_NAMESPACE` from env
- If neither set: log info, return (extension disabled)
- Set `page_config["sparkMonitorEnabled"] = "true"`
- Call `setup_handlers()` with both URLs + `KBASE_AUTH_URL` + `SPARK_MASTER_PORT`
- Pattern: CoreUI's `__init__.py`

**`berdl_spark_monitor/handlers.py`** — The core proxy logic:

- `SparkMonitorBaseHandler(JupyterHandler)` base class:
  - `kbase_token` property reads `kbase_session`/`kbase_session_backup` cookie
  - `resolve_username(token)` validates against Auth2 `GET /api/V2/me`, caches result 5min
  - `spark_master_base_url(username)` constructs `http://spark-master-{sanitized}.{ns}:{port}`
  - `sanitize_k8s_name()` — replicate exactly from `spark_cluster_manager/src/spark_manager.py:21-47`

- `ClusterStatusHandler` — proxies to `{cluster_manager_url}/clusters` with Bearer token
- `SparkClusterSummaryHandler` — proxies to Spark Master `/json/`
- `SparkExecutorsHandler` — resolves app ID from `/json/` (cached 60s), proxies to `.../executors`
- `SparkStagesHandler` — same app ID cache, proxies to `.../stages`

- `setup_handlers(web_app, ...)` — registers routes conditionally (cluster manager routes only if URL set, Spark Master routes only if namespace set)
- Pattern: access request extension's `handlers.py` setup_handlers

**Key files to reference:**

- `spark_cluster_manager/src/spark_manager.py:21-47` — canonical `sanitize_k8s_name`
- `plans/01-spark-cluster-status-chip.md` section 2.4 — proxy handler implementation
- `BERDL_JupyterLab_CoreUI/berdl_jupyterlab_coreui/__init__.py` — server ext registration

### Phase 2: Status Bar Widget

**`src/constants.ts`** — Extension IDs, LabIcon (gauge SVG), polling intervals.

**`src/types.ts`** — `IDeploymentStatus`, `ISparkClusterStatus` (from cluster manager), plus `ISparkMasterSummary`, `IExecutorSummary`, `IStageSummary` (from Spark REST API).

**`src/api/sparkApi.ts`** — Thin fetch wrappers using `URLExt.join(PageConfig.getBaseUrl(), ...)`. No auth headers needed (same-origin cookie). Custom `SparkApiError` class.

**`src/hooks/useClusterStatus.ts`** — Polls cluster manager status every 30s. Always active (not visibility-aware — this drives the ambient status bar).

**`src/components/StatusBarWidget.tsx`** — Derives display state from cluster status:
| Condition | Label | Dot Color |
|-----------|-------|-----------|
| Loading | `Spark: ...` | muted |
| Fetch error / no data | `Spark` | muted |
| `error === true` | `Spark: Error` | red |
| `!master.exists` | `Spark: No Cluster` | muted |
| Both ready | `Spark: Ready (N/N)` | green |
| Else | `Spark: Starting (N/M)` | amber |

Clickable — fires `COMMAND_OPEN_PANEL` to open sidebar.

**`src/index.ts`** (phase 2 portion):

- Check `PageConfig.getOption('sparkMonitorEnabled') !== 'true'` → early return
- Create `QueryClient`
- Register `COMMAND_OPEN_PANEL` command → `app.shell.activateById(PANEL_ID)`
- `IStatusBar` in `optional` requires array
- If statusBar available: create `ReactWidget` with `StatusBarWidget`, call `statusBar.registerStatusItem()` with `align: 'left'`

**Key pattern:** `IStatusBar` is optional (graceful degradation). Status bar widget is a `ReactWidget` wrapped in `QueryClientProvider`.

### Phase 3: Sidebar Panel — Cluster Overview

**Panel visibility tracking** — Lumino widgets have `onAfterShow`/`onAfterHide` lifecycle methods (not signals). Subclass `Panel`:

```typescript
class SparkMonitorPanel extends Panel {
  private _visibilityCallback: ((v: boolean) => void) | null = null;
  setVisibilityCallback(cb: (v: boolean) => void) {
    this._visibilityCallback = cb;
  }
  protected onAfterShow(msg: Message): void {
    super.onAfterShow(msg);
    this._visibilityCallback?.(true);
  }
  protected onAfterHide(msg: Message): void {
    super.onAfterHide(msg);
    this._visibilityCallback?.(false);
  }
}
```

React gets visibility via `SparkVisibilityContext` (React context, default `false`).

**`src/index.ts`** (phase 3 addition):

- Create `SparkMonitorPanel` subclass instance
- Set icon, ID, add to `'left'` with `rank: 1100` (after task browser at 1000)
- Register with `ILayoutRestorer`
- Wire visibility callback to React state

**`src/components/SparkMonitorPanel.tsx`** — Root component:

```
<SparkVisibilityContext.Provider value={isVisible}>
  <ClusterOverview />
  <CollapsibleSection title="Executors" defaultOpen>
    <ExecutorTable />
  </CollapsibleSection>
  <CollapsibleSection title="Active Stages" defaultOpen>
    <ActiveStages />
  </CollapsibleSection>
  <CollapsibleSection title="Recent Queries" defaultOpen={false}>
    <QueryHistory />
  </CollapsibleSection>
</SparkVisibilityContext.Provider>
```

**`src/hooks/useClusterSummary.ts`** — Polls Spark Master `/json/` every 30s. `enabled: isVisible`.

**`src/components/ClusterOverview.tsx`** — Worker count + two `ResourceBar` components (cores, memory).

**`src/components/ResourceBar.tsx`** — Progress bar with color thresholds: green <70%, amber 70-90%, red >90%. Pure CSS transitions.

**`src/components/CollapsibleSection.tsx`** — Chevron + title, `aria-expanded`, local `useState`.

### Phase 4: Sidebar — Executors + Stages

**`src/hooks/useExecutors.ts`** — Visibility-aware + adaptive polling:

- `enabled: isVisible`
- 10s when any executor has `activeTasks > 0`, 30s otherwise
- Handle 404 (no active app) gracefully → show "No active Spark session"

**`src/hooks/useStages.ts`** — Same pattern:

- 5s when any stage is ACTIVE, 30s otherwise
- Splits response into `active` (status=ACTIVE) and `recent` (COMPLETE/FAILED, last 20, newest first)

**`src/components/ExecutorTable.tsx`** — Compact table: ID, Active Tasks, Memory (bar), Disk Used, Shuffle R/W, GC Time. Red text for disk > 0, amber for GC > 10% of uptime.

**`src/components/SpillWarning.tsx`** — Banner above executor table when any executor spills. Lists executor IDs. Amber left border.

**`src/components/ActiveStages.tsx`** — Progress bar per active stage (tasks completed/total). Secondary line with I/O metrics.

**`src/components/QueryHistory.tsx`** — Last 20 stages. Green check/red X for status. Compact single-line rows, click to expand.

**`src/utils/format.ts`** — `formatBytes(bytes)` and `formatDuration(ms)`, pure functions.

### Phase 5: Polish

- Theme testing (light + dark)
- All CSS uses `var(--jp-*)` variables
- Error state coverage: cluster unreachable, no active app, no token, proxy timeout
- Verify sidebar polling pauses when hidden, resumes on show
- Verify status bar continues polling independently
- No console errors

---

## Polling Summary

| Hook                | Drives                  | Active Rate | Idle Rate | Visibility-aware |
| ------------------- | ----------------------- | ----------- | --------- | ---------------- |
| `useClusterStatus`  | Status bar              | 30s         | 30s       | No (always)      |
| `useClusterSummary` | Sidebar overview        | 30s         | 30s       | Yes              |
| `useExecutors`      | Executor table          | 10s         | 30s       | Yes              |
| `useStages`         | Active stages + history | 5s          | 30s       | Yes              |

React Query pauses `refetchInterval` when browser tab is in background (default).

## Environment Variables

| Variable                        | Required                 | Default                              | Purpose                              |
| ------------------------------- | ------------------------ | ------------------------------------ | ------------------------------------ |
| `SPARK_CLUSTER_MANAGER_API_URL` | Either this or namespace | —                                    | Cluster manager for status bar       |
| `BERDL_JUPYTERHUB_NAMESPACE`    | Either this or above     | —                                    | K8s namespace for Spark Master proxy |
| `KBASE_AUTH_URL`                | No                       | `https://ci.kbase.us/services/auth/` | Auth2 validation                     |
| `SPARK_MASTER_PORT`             | No                       | `8090`                               | Spark Master UI port                 |

## Dependencies

**TypeScript** (additions vs template base): `@jupyterlab/statusbar ^4.0.0`, `@jupyterlab/statedb ^4.0.0`, `@tanstack/react-query ^5.68.0`, `react-dom ^18.0.0`
**Python**: `jupyter_server>=2.4.0` (from template), `httpx>=0.28.0` (added)

## Verification

1. Set `SPARK_CLUSTER_MANAGER_API_URL` and `BERDL_JUPYTERHUB_NAMESPACE` in env
2. `pip install -e ".[dev]" && jlpm build && jupyter labextension develop . --overwrite`
3. Start JupyterLab — status bar should show "Spark: Ready (4/4)" or appropriate state
4. Click status bar item — sidebar panel should open
5. Run a Spark query in a notebook — Active Stages should show progress, executor table should update at 10s
6. Trigger disk spill — SpillWarning banner should appear
7. Close sidebar — verify network tab shows only `/status` polling continues, not `/cluster`/`/executors`/`/stages`
8. Unset both env vars — extension should not activate, no errors

---

## Completion Notes

### Scaffolding (done)

- Generated via `copier copy` from `jupyterlab/extension-template` (v4.5.2, `frontend-and-server` kind)
- Applied KBase overlay via `copier copy` from `kbase/kbase-jlab-overlay` + `uv run post_copy.py`
- Fixed malformed TOML leftover in `pyproject.toml` (bare array from template not cleaned by `post_copy.py`)
- Kept template's `routes.py` filename instead of renaming to `handlers.py`
- Created `.copier-answers-kbase.yml` manually (overlay task errored on first run due to `python` not on PATH)

### Files stubbed (all phases 1-4)

- **Python server extension**: `__init__.py` rewritten (env→page_config pattern from CoreUI), `routes.py` rewritten (4 proxy handlers + `sanitize_k8s_name` replicated from spark_cluster_manager)
- **TypeScript core**: `constants.ts`, `types.ts`, `index.ts` (full plugin with status bar + sidebar + command + layout restorer)
- **API layer**: `src/api/sparkApi.ts` (fetch wrappers for 4 proxy endpoints)
- **Hooks**: `useClusterStatus` (always-on 30s), `useClusterSummary` (visibility-aware 30s), `useExecutors` (adaptive 10s/30s), `useStages` (adaptive 5s/30s with active/recent split)
- **Components**: `StatusBarWidget`, `SparkMonitorPanel` (with `SparkVisibilityContext`), `ClusterOverview`, `ResourceBar`, `CollapsibleSection`, `ExecutorTable`, `SpillWarning`, `ActiveStages`, `QueryHistory`
- **Utilities**: `formatBytes()`, `formatDuration()`
- **Styles**: `style/base.css` with full component styles using `var(--jp-*)` theme variables
- **Config**: `pyproject.toml` (added `httpx>=0.28.0`), `package.json` (added `@jupyterlab/statusbar`, `@jupyterlab/statedb`, `@jupyterlab/apputils`, `@lumino/widgets`, `@tanstack/react-query`, `react`, `react-dom`)

### Deviations from plan

- `handlers.py` → kept as `routes.py` (template convention)
- `request.ts` (template file) deleted, replaced by `src/api/sparkApi.ts`
- `tsconfig.json` updated by linter to add `"lib": ["DOM", "ES2018", "ES2020.Intl"]`

### Remaining for implementation

- Phase 5 (Polish): theme testing, error state coverage, verify polling behavior, no console errors
- `jlpm install && jlpm build` to validate TypeScript compiles
- Integration testing with live Spark cluster
